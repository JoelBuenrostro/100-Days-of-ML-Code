## Anaconda Distribution :
Anaconda Distribution is a free, easy-to-install package manager, environment manager, and Python distribution with a collection of 1,500+ open source packages with free community support. Anaconda is platform-agnostic, so you can use it whether you are on Windows, macOS, or Linux.
[Homepage](https://www.anaconda.com/)

## K Means Clustering :
k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.
[Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)

## K-Nearest neighbors :
In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression
[Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

## Kaggle :
Inside Kaggle you’ll find all the code & data you need to do your data science work. Use over 19,000 public datasets and 200,000 public notebooks to conquer any analysis in no time.
[Homepage](https://www.kaggle.com/)

## KDE :
In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.

![kde](https://wikimedia.org/api/rest_v1/media/math/render/svg/f3b09505158fb06033aabf9b0116c8c07a68bf31)

## Law of large numbers :
In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed.
[Wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)

## Linear regression :
In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).

![Regression analysis](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/330px-Linear_regression.svg.png)

[Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)

## Random forest :
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
[Wikipedia](https://en.wikipedia.org/wiki/Random_forest)

## Support vector machines :
In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.
[Wikipedi](https://en.wikipedia.org/wiki/Support-vector_machine)

## Virtual environment :
Self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages.
[Python Docs](https://docs.python.org/3/tutorial/venv.html)